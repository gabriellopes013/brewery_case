# Brewery Data Pipeline - Engenharia de Dados com Airflow, Spark e AWS

Este projeto implementa um pipeline de engenharia de dados baseado na arquitetura em camadas **Bronze â†’ Silver â†’ Gold**, utilizando **Astronomer Airflow** para orquestraÃ§Ã£o, **PySpark** para transformaÃ§Ã£o, **AWS S3** para armazenamento, **AWS Glue** como catÃ¡logo de dados e **AWS Athena** para consulta via SQL.
---
## Instalacoes necessarias
- [Docker](https://www.docker.com/get-started/)
- [Astronomer CLI](https://docs.astronomer.io/astro/cli/install-cli)

---
## Tecnologias Utilizadas

- **Astronomer Airflow** â€“ OrquestraÃ§Ã£o de tarefas
- **PySpark** â€“ Processamento de dados em larga escala
- **AWS S3** â€“ Armazenamento das camadas Bronze, Silver e Gold
- **AWS Glue** â€“ CatÃ¡logo de dados para uso com Athena
- **AWS Athena** â€“ Consulta de dados usando SQL diretamente no S3
- **Docker** â€“ ContainerizaÃ§Ã£o do ambiente
- **Pytest** â€“ Testes unitÃ¡rios para validaÃ§Ã£o de cada camada
- **dotenv** â€“ Gerenciamento de variÃ¡veis sensÃ­veis

---

## Arquitetura em Camadas

```text
API PÃºblica
   â†“
[ Bronze ] â†’ Dados brutos
   â†“
[ Silver ] â†’ Dados limpos, normalizados, com timestamp
   â†“
[ Gold ] â†’ Dados prontos para anÃ¡lise com Athena
```

---

## Estrutura do Projeto

```
brewery_case/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ brewery_dag.py                # DAG do Airflow
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ raw_ingest.py             # IngestÃ£o da API para Bronze
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ silver_ingestion.py       # TransformaÃ§Ãµes para Silver
â”‚   â””â”€â”€ gold/
â”‚       â””â”€â”€ gold_ingestion.py         # AgregaÃ§Ãµes e partiÃ§Ãµes para Gold
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ bronze/
|   |   â””â”€â”€ test_raw_ingestion.py
â”‚   â”œâ”€â”€ silver/
|   |   â””â”€â”€ test_silver.py
â”‚   â””â”€â”€ gold/
|       â””â”€â”€ test_gold.py
â”œâ”€â”€ Dockerfile                        # Imagem customizada com PySpark e Java
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                              # VariÃ¡veis sensÃ­veis
```

---

## ExecuÃ§Ã£o do Pipeline

A DAG do Airflow Ã© composta pelas seguintes tarefas:

1. `run_bronze` â†’ IngestÃ£o da API e gravaÃ§Ã£o no S3
2. `test_bronze` â†’ ValidaÃ§Ã£o dos dados brutos com Pytest
3. `run_silver` â†’ Limpeza e transformaÃ§Ã£o da Bronze
4. `test_silver` â†’ ValidaÃ§Ã£o dos dados tratados
5. `run_gold` â†’ AgregaÃ§Ãµes e preparaÃ§Ã£o para anÃ¡lise
6. `test_gold` â†’ ValidaÃ§Ã£o final antes da anÃ¡lise

As tarefas estÃ£o encadeadas com dependÃªncias, garantindo execuÃ§Ã£o ordenada e segura.

---

##  Testes Automatizados

Cada camada possui seus testes organizados:

```bash
# Testes individuais
pytest tests/bronze/test_raw_ingestion.py
pytest tests/silver/test_silver.py
pytest tests/gold/test_gold.py
```

Os testes sÃ£o executados automaticamente via Airflow entre as etapas do pipeline.

---

##  Como Rodar com Astro

```bash
# InicializaÃ§Ã£o com Astro
astro dev start
```

---

## ğŸ” Arquivo `.env`

Crie um arquivo `.env` na raiz com as credenciais da AWS:

```env
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=True
AIRFLOW__SMTP__SMTP_SSL=False
AIRFLOW__SMTP__SMTP_USER=Seu email
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_PASSWORD=Sua Senha 
AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp
AWS_ACCESS_KEY_ID=SEU_ACCESS_KEY
AWS_SECRET_ACCESS_KEY=SUA_SECRET_KEY
AWS_S3_BUCKET=seu bucket
```
obs: caso for gmail faÃ§a uma senha para aplicativo e habilite autenticaÃ§Ã£o em dois fatores

---

## ğŸ“Š Consulta com AWS Athena

1. Acesse o serviÃ§o **AWS Glue**
2. Crie um **Crawler** apontando para `s3://<seu-bucket>/gold/`
3. Rode o crawler e crie uma tabela no Glue Data Catalog
4. VÃ¡ atÃ© o **AWS Athena** e execute:

```sql
SELECT country, COUNT(*) as total
FROM gold
GROUP BY country
ORDER BY total DESC;
```

---

## Detalhes de Cada Camada

###  Bronze
- Dados coletados da API pÃºblica Open Brewery DB
- Formato: `.parquet`
- Local: `s3a://<bucket>/bronze/`

###  Silver
- Dados tratados: trim em colunas, uppercase, remoÃ§Ã£o de nulos
- Adicionado timestamp de processamento
- Particionado por `country`
- Local: `s3a://<bucket>/silver/`

### Gold
- Dados prontos para anÃ¡lise
- Pode conter agregaÃ§Ãµes como total por paÃ­s e tipo de brewery
- Particionado para performance
- Local: `s3a://<bucket>/gold/`

---

## Melhorias Futuras

- CI/CD com GitHub Actions
- Observabilidade com Datadog ou Prometheus
- ValidaÃ§Ã£o de schema com Great Expectations
- Uso de Delta Lake ou Iceberg para versionamento

---
# Screenshots

### Airflow - DAG Pipeline
![Airflow DAG](images/airflow_dag.png)

### AWS S3 - Camadas de Dados
![S3 Buckets](images/s3_buckets.png)

### AWS Glue - CatÃ¡logo de Dados
![Glue Catalog](images/glue_catalog.png)

### AWS Athena - Consulta na Camada Gold
![Athena Query](images/athena_query.png)

---
## ğŸ‘¨â€ğŸ’» Autor

Gabriel Almeida  
ğŸ“§ gabriel.almeida99.job@gmail.com  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/gabriel-almeida-129343190/)

---

## ğŸ ConclusÃ£o

Este projeto demonstra um pipeline de engenharia de dados completo, com ingestÃ£o, transformaÃ§Ã£o e preparaÃ§Ã£o para anÃ¡lise via AWS Athena. A modularidade permite fÃ¡cil manutenÃ§Ã£o e escalabilidade, alÃ©m de boas prÃ¡ticas com testes e orquestraÃ§Ã£o profissional com Astronomer Airflow.